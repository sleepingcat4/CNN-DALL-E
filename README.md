## What’s CNN?

Within Deep Learning, a Convolutional Neural Network or CNN is a type of artiﬁcial neural network, which is widely used for image/object recognition and classiﬁcation. Deep Learning thus recognizes objects in an image by using a CNN.

## What’s DALL-E?

DALL-E and DALL-E 2 are transformer models developed by OpenAI to generate digital images from natural language descriptions. Its name is a portmanteau of WALL-E and Salvador Dalí.

## Project Objective

The project focused on proving that DALL-E generated images can be used as high-quality Synthetic image datasets for training Artiﬁcial Intelligence for Image Classiﬁcation tasks. With that objective/hypothesis in mind, our team at alone! ai trained a CNN oriented AI model using Pytorch. All the images for the AI model were taken from DALL-E.

## Reasons to do so?

Recently from the emergence of text-to-Image generating models such as DALL-E 2 and Google Imagen, we have found out that these models can be used for generating image datasets of choice. FInding quality Image Datasets for training AI models can be hard and expensive. Not everyone has the resources to aﬀord costly high-quality image datasets, mainly when you’re a student researcher. Our researcher hypothesised by looking at high-quality images from text-to-image models as DALL-E that these models enable a cheap way to train AI models with any sort of Image datasets of your choice. To conﬁrm our hunch, we created a simple CNN model using Pytorch to train it on DALL-E generated Oil Paintings and Pencil Drawings images. Successfully our model was able to successfully train itself from DALL-E generated image datasets.
DALL-E is still in beta mode. Only few researchers and artists have access to it. We hope that, once the API is made public, OpenAI will look into the prospect of DALL-E generated high-quality image datasets.

## About the Dataset

There were total 30 images and training and test images were kept same for simplicity. And two classes/labels i.e. Oil Paintings and Pencil Drawings each having 15 images respectively. Furthermore, the dataset consisted variations a single image that was generated by DALL-E than 15 images being completely different. It was done for experimental purpose both of DALL-E and the model to find out how well the model can perform in challenging conditions where images only have a slight deviation. Espically it was focused into the medical section. Where images mostly have slight variations. And these dataset serves as a good proof that DALL-E can be creatively used for harnessiharnessing virtually any situation and generate image on it.

## Outcomes

From our model that was trained on 15 Oil painting images and 15 Pencil Painting images we were able to attain a 30% training accuracy and 28% testing accuracy on average. Given to our image dataset, which is a good ratio as we consider.

## Drawbacks

Since we had tried to prove our hypothesis, we didn’t use a large dataset rather a small one. Considering the outcomes we’re sure that, with more DALL-E 
images, about 125 images per label, a CNN model can attain 100% accuracy
from DALL-E generated image datasets.

## Interesting points

We propose further investigation in GPT-3 for its ability to create Synthetic data.And we’re curious how OpenAI could handle people who’re generating datasets
from who aren’t. Besides that, It’s possible to train a model using one single image with just slight variations using the variable option. I have trained my model that way to ﬁnd interesting outcomes.

Apart from these, DALL-E generated high-quality image datasets ensures that, there will be virtually no dodgy images that needs to be removed.

## Yay facts

Further we have deployed a readily usable web-app created using the DALL-E generated image dataset same as this project to deploy an image classiﬁer using lobe.ai using Microsoft Azure. Check it out! http://dalle.azurewebsites.net/

(Python 3.8 has been used)

### EPOCH Data

Epoch: 1/10     Training Loss: 2.0474   Training Accuracy: 1.0000
Epoch: 1/10     Testing Loss: 0.4270    Testing Accuracy: 19.0000
Saving the best model
Epoch: 2/10     Training Loss: 0.0026   Training Accuracy: 30.0000
Epoch: 2/10     Testing Loss: 0.3340    Testing Accuracy: 25.0000
Saving the best model
Epoch: 3/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 3/10     Testing Loss: 0.2798    Testing Accuracy: 28.0000
Saving the best model
Epoch: 4/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 4/10     Testing Loss: 0.2575    Testing Accuracy: 28.0000
Epoch: 5/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 5/10     Testing Loss: 0.2699    Testing Accuracy: 28.0000
Epoch: 6/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 6/10     Testing Loss: 0.3074    Testing Accuracy: 25.0000
Epoch: 7/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 7/10     Testing Loss: 0.3672    Testing Accuracy: 22.0000
Epoch: 8/10     Training Loss: 0.0000   Training Accuracy: 30.0000
Epoch: 8/10     Testing Loss: 0.5077    Testing Accuracy: 21.0000
Epoch: 9/10     Training Loss: 0.0001   Training Accuracy: 30.0000
Epoch: 9/10     Testing Loss: 0.5695    Testing Accuracy: 19.0000
Epoch: 10/10    Training Loss: 0.0002   Training Accuracy: 30.0000
Epoch: 10/10    Testing Loss: 0.4676    Testing Accuracy: 21.0000
